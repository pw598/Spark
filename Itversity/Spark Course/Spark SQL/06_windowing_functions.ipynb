{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing Functions\n",
    "\n",
    "As part of this section we will primarily talk about Windowing Functions. These are also known as Analytic Functions in Databases like Oracle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/wkYC9crqHH8?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/wkYC9crqHH8?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prepare HR Database\n",
    "* Overview of Windowing Functions\n",
    "* Aggregations using Windowing Functions\n",
    "* Getting LEAD and LAG values\n",
    "* Getting first and last values\n",
    "* Ranking using Windowing Functions\n",
    "* Understanding order of execution of SQL\n",
    "* Overview of Nested Sub Queries\n",
    "* Filtering - Window Function Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    config(\"spark.sql.warehouse.dir\", s\"/user/${username}/warehouse\").\n",
    "    enableHiveSupport.\n",
    "    appName(s\"${username} | Spark SQL - Windowing Functions\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SET spark.sql.shuffle.partitions=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare HR Database\n",
    "\n",
    "Let us prepare HR database with **EMPLOYEES** Table. We will be using this for some of the examples as well as exercises related to Window Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ooxBTw_UU3U?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ooxBTw_UU3U?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create Database **itversity_hr** (replace itversity with your OS User Name)\n",
    "* Create table **employees** in **itversity_hr** database.\n",
    "* Load data into the table.\n",
    "\n",
    "First let us start with creating the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DROP DATABASE itversity_hr CASCADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE DATABASE itversity_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT current_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the database is created, let us go ahead and add table to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE employees (\n",
    "  employee_id     int,\n",
    "  first_name      varchar(20),\n",
    "  last_name       varchar(25),\n",
    "  email           varchar(25),\n",
    "  phone_number    varchar(20),\n",
    "  hire_date       date,\n",
    "  job_id          varchar(10),\n",
    "  salary          decimal(8,2),\n",
    "  commission_pct  decimal(2,2),\n",
    "  manager_id      int,\n",
    "  department_id   int\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the data and validate the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "LOAD DATA LOCAL INPATH '/data/hr_db/employees' \n",
    "INTO TABLE employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM employees LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT employee_id, department_id, salary FROM employees LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT count(1) FROM employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Windowing Functions\n",
    "\n",
    "Let us get an overview of Analytics or Windowing Functions in Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/psc34WIg3ew?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/psc34WIg3ew?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aggregate Functions (`sum`, `min`, `max`, `avg`)\n",
    "* Window Functions (`lead`, `lag`, `first_value`, `last_value`)\n",
    "* Rank Functions (`rank`, `dense_rank`, `row_number` etc)\n",
    "* For all the functions we use `OVER` clause.\n",
    "* For aggregate functions we typically use `PARTITION BY`\n",
    "* For global ranking and windowing functions we can use `ORDER BY sorting_column` and for ranking and windowing with in a partition or group we can use `PARTITION BY partition_column ORDER BY sorting_column`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT employee_id, department_id, salary FROM employees LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT employee_id, department_id, salary,\n",
    "    count(1) OVER (PARTITION BY department_id) AS employee_count,\n",
    "    rank() OVER (ORDER BY salary DESC) AS rnk,\n",
    "    lead(employee_id) OVER (PARTITION BY department_id ORDER BY salary DESC) AS lead_emp_id,\n",
    "    lead(salary) OVER (PARTITION BY department_id ORDER BY salary DESC) AS lead_emp_sal\n",
    "FROM employees\n",
    "ORDER BY employee_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations using Windowing Functions\n",
    "\n",
    "Let us see how we can perform aggregations with in a partition or group using Windowing/Analytics Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/peDMzBredoU?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/peDMzBredoU?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For simple aggregations where we have to get grouping key and aggregated results we can use **GROUP BY**.\n",
    "* If we want to get the raw data along with aggregated results, then using **GROUP BY** is not possible or overly complicated.\n",
    "* Using aggregate functions with **OVER** Clause not only simplifies the process of writing query, but also better with respect to performance.\n",
    "* Let us take an example of getting employee salary percentage when compared to department salary expense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT employee_id, department_id, salary \n",
    "FROM employees \n",
    "ORDER BY department_id, salary\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let us write the query using `GROUP BY` approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT department_id,\n",
    "       sum(salary) AS department_salary_expense\n",
    "FROM employees\n",
    "GROUP BY department_id\n",
    "ORDER BY department_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT e.employee_id, e.department_id, e.salary,\n",
    "       ae.department_salary_expense,\n",
    "       ae.avg_salary_expense\n",
    "FROM employees e JOIN (\n",
    "     SELECT department_id, \n",
    "            sum(salary) AS department_salary_expense,\n",
    "            avg(salary) AS avg_salary_expense\n",
    "     FROM employees\n",
    "     GROUP BY department_id\n",
    ") ae\n",
    "ON e.department_id = ae.department_id\n",
    "ORDER BY department_id, salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let us see how we can get it using Analytics/Windowing Functions. \n",
    "\n",
    "* We can use all standard aggregate functions such as `count`, `sum`, `min`, `max`, `avg` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT e.employee_id, e.department_id, e.salary,\n",
    "       sum(e.salary) \n",
    "         OVER (PARTITION BY e.department_id)\n",
    "         AS department_salary_expense\n",
    "FROM employees e\n",
    "ORDER BY e.department_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT e.employee_id, e.department_id, e.salary,\n",
    "    sum(e.salary) OVER (PARTITION BY e.department_id) AS sum_sal_expense,\n",
    "    avg(e.salary) OVER (PARTITION BY e.department_id) AS avg_sal_expense,\n",
    "    min(e.salary) OVER (PARTITION BY e.department_id) AS min_sal_expense,\n",
    "    max(e.salary) OVER (PARTITION BY e.department_id) AS max_sal_expense,\n",
    "    count(e.salary) OVER (PARTITION BY e.department_id) AS cnt_sal_expense\n",
    "FROM employees e\n",
    "ORDER BY e.department_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tables to get daily revenue\n",
    "\n",
    "Let us create couple of tables which will be used for the demonstrations of Windowing and Ranking functions.\n",
    "\n",
    "* We have **ORDERS** and **ORDER_ITEMS** tables.\n",
    "* Let us take care of computing daily revenue as well as daily product revenue.\n",
    "* As we will be using same data set several times, let us create the tables to pre compute the data.\n",
    "* **daily_revenue** will have the **order_date** and **revenue**, where data is aggregated using **order_date** as partition key.\n",
    "* **daily_product_revenue** will have **order_date**, **order_item_product_id** and **revenue**. In this case data is aggregated using **order_date** and **order_item_product_id** as partition keys.\n",
    "\n",
    "Let us create table to compute daily revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS daily_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE daily_revenue\n",
    "AS\n",
    "SELECT o.order_date,\n",
    "       round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * \n",
    "FROM daily_revenue\n",
    "ORDER BY order_date\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create table to compute daily product revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS daily_product_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE daily_product_revenue\n",
    "AS\n",
    "SELECT o.order_date,\n",
    "       oi.order_item_product_id,\n",
    "       round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date, oi.order_item_product_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * \n",
    "FROM daily_product_revenue\n",
    "ORDER BY order_date, order_item_product_id\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative or Moving Aggregations\n",
    "\n",
    "Let us understand how we can take care of cumulative or moving aggregations using Spark SQL.\n",
    "* When it comes to Windowing or Analytic Functions we can also specify window using `ROWS BETWEEN` clause.\n",
    "* We can leverage `ROWS BETWEEN` for cumulative aggregations or moving aggregations.\n",
    "* Here is an example of cumulative sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT e.employee_id, e.department_id, e.salary,\n",
    "    sum(e.salary) OVER (\n",
    "        PARTITION BY e.department_id\n",
    "        ORDER BY e.salary DESC\n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "    ) AS sum_sal_expense\n",
    "FROM employees e\n",
    "ORDER BY e.department_id, e.salary DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT t.*,\n",
    "    round(sum(t.revenue) OVER (\n",
    "        PARTITION BY date_format(order_date, 'yyyy-MM')\n",
    "        ORDER BY order_date\n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "    ), 2) AS cumulative_daily_revenue\n",
    "FROM daily_revenue t\n",
    "ORDER BY date_format(order_date, 'yyyy-MM'),\n",
    "    order_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT t.*,\n",
    "    round(sum(t.revenue) OVER (\n",
    "        PARTITION BY date_format(order_date, 'yyyy-MM')\n",
    "        ORDER BY order_date\n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "    ), 2) AS cumulative_daily_revenue\n",
    "FROM daily_revenue t\n",
    "ORDER BY date_format(order_date, 'yyyy-MM'), \n",
    "    order_date\n",
    "\"\"\").\n",
    "    show(100, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is an example for moving sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT t.*,\n",
    "    round(sum(t.revenue) OVER (\n",
    "        ORDER BY order_date\n",
    "        ROWS BETWEEN 3 PRECEDING AND CURRENT ROW\n",
    "    ), 2) AS moving_3day_revenue\n",
    "FROM daily_revenue t\n",
    "ORDER BY order_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT t.*,\n",
    "        round(sum(t.revenue) OVER (\n",
    "            ORDER BY order_date\n",
    "            ROWS BETWEEN 3 PRECEDING AND CURRENT ROW\n",
    "        ), 2) AS moving_3day_revenue\n",
    "    FROM daily_revenue t\n",
    "    ORDER BY order_date\n",
    "\"\"\").\n",
    "    show(30, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT t.*,\n",
    "    round(sum(t.revenue) OVER (\n",
    "        PARTITION BY date_format(order_date, 'yyyy-MM')\n",
    "        ORDER BY order_date\n",
    "        ROWS BETWEEN 3 PRECEDING AND CURRENT ROW\n",
    "    ), 2) AS moving_3day_revenue\n",
    "FROM daily_revenue t\n",
    "ORDER BY date_format(order_date, 'yyyy-MM'),\n",
    "    order_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT t.*,\n",
    "    round(sum(t.revenue) OVER (\n",
    "        PARTITION BY date_format(order_date, 'yyyy-MM')\n",
    "        ORDER BY order_date\n",
    "        ROWS BETWEEN 3 PRECEDING AND CURRENT ROW\n",
    "    ), 2) AS moving_3day_revenue\n",
    "FROM daily_revenue t\n",
    "ORDER BY date_format(order_date, 'yyyy-MM'), \n",
    "    order_date\n",
    "\"\"\").\n",
    "    show(100, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LEAD or LAG\n",
    "\n",
    "Let us understand LEAD and LAG functions to get column values from following or prior records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/BhWIslXAebo?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/BhWIslXAebo?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the example where we can get prior or following records based on **ORDER BY** within **OVER** Clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM daily_revenue\n",
    "ORDER BY order_date DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT t.*,\n",
    "  lead(order_date) OVER (ORDER BY order_date DESC) AS prior_date,\n",
    "  lead(revenue) OVER (ORDER BY order_date DESC) AS prior_revenue,\n",
    "  lag(order_date) OVER (ORDER BY order_date) AS lag_prior_date,\n",
    "  lag(revenue) OVER (ORDER BY order_date) AS lag_prior_revenue\n",
    "FROM daily_revenue AS t\n",
    "ORDER BY order_date DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT t.*,\n",
    "  lead(order_date) OVER (ORDER BY order_date DESC) AS prior_date,\n",
    "  lead(revenue) OVER (ORDER BY order_date DESC) AS prior_revenue\n",
    "FROM daily_revenue AS t\n",
    "ORDER BY order_date\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pass number of rows as well as default values for nulls as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT t.*,\n",
    "  lead(order_date, 7) OVER (ORDER BY order_date DESC) AS prior_date,\n",
    "  lead(revenue, 7) OVER (ORDER BY order_date DESC) AS prior_revenue\n",
    "FROM daily_revenue t\n",
    "ORDER BY order_date DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT t.*,\n",
    "  lead(order_date, 7) OVER (ORDER BY order_date DESC) AS prior_date,\n",
    "  lead(revenue, 7) OVER (ORDER BY order_date DESC) AS prior_revenue\n",
    "FROM daily_revenue t\n",
    "ORDER BY order_date\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT t.*,\n",
    "  lead(order_date, 7, 'NA') OVER (ORDER BY order_date DESC) AS prior_date,\n",
    "  lead(revenue, 7, 0) OVER (ORDER BY order_date DESC) AS prior_revenue\n",
    "FROM daily_revenue t\n",
    "ORDER BY order_date\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how we can get prior or following records with in a group based on particular order.\n",
    "\n",
    "Here is the example where we can get prior or following records based on **PARTITION BY** and then **ORDER BY** Clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DESCRIBE daily_product_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM daily_product_revenue \n",
    "ORDER BY order_date, revenue DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT t.*,\n",
    "  LEAD(order_item_product_id) OVER (\n",
    "    PARTITION BY order_date \n",
    "    ORDER BY revenue DESC\n",
    "  ) next_product_id,\n",
    "  LEAD(revenue) OVER (\n",
    "    PARTITION BY order_date \n",
    "    ORDER BY revenue DESC\n",
    "  ) next_revenue\n",
    "FROM daily_product_revenue t\n",
    "ORDER BY order_date, revenue DESC\n",
    "LIMIT 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT t.*,\n",
    "      LEAD(order_item_product_id) OVER (\n",
    "        PARTITION BY order_date \n",
    "        ORDER BY revenue DESC\n",
    "      ) next_product_id,\n",
    "      LEAD(revenue) OVER (\n",
    "        PARTITION BY order_date \n",
    "        ORDER BY revenue DESC\n",
    "      ) next_revenue\n",
    "    FROM daily_product_revenue t\n",
    "    ORDER BY order_date, revenue DESC\n",
    "\"\"\").\n",
    "    show(100, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pass number of rows as well as default values for nulls as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT t.*,\n",
    "  LEAD(order_item_product_id) OVER (\n",
    "    PARTITION BY order_date ORDER BY revenue DESC\n",
    "  ) next_product_id,\n",
    "  LEAD(revenue, 1, 0) OVER (\n",
    "    PARTITION BY order_date ORDER BY revenue DESC\n",
    "  ) next_revenue\n",
    "FROM daily_product_revenue t\n",
    "ORDER BY order_date, revenue DESC\n",
    "LIMIT 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT t.*,\n",
    "      LEAD(order_item_product_id) OVER (\n",
    "        PARTITION BY order_date ORDER BY revenue DESC\n",
    "      ) next_product_id,\n",
    "      LEAD(revenue, 1, 0) OVER (\n",
    "        PARTITION BY order_date ORDER BY revenue DESC\n",
    "      ) next_revenue\n",
    "    FROM daily_product_revenue t\n",
    "    ORDER BY order_date, revenue DESC\n",
    "    LIMIT 100\n",
    "\"\"\").\n",
    "    show(100, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting first and last values\n",
    "\n",
    "Let us see how we can get first and last value based on the criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/0kJ-ZdOJnKQ?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/0kJ-ZdOJnKQ?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the example of using first_value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT t.*,\n",
    "  first_value(order_item_product_id) OVER (\n",
    "    PARTITION BY order_date ORDER BY revenue DESC\n",
    "  ) first_product_id,\n",
    "  first_value(revenue) OVER (\n",
    "    PARTITION BY order_date ORDER BY revenue DESC\n",
    "  ) first_revenue\n",
    "FROM daily_product_revenue t\n",
    "ORDER BY order_date, revenue DESC\n",
    "LIMIT 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT t.*,\n",
    "      first_value(order_item_product_id) OVER (\n",
    "        PARTITION BY order_date ORDER BY revenue DESC\n",
    "      ) first_product_id,\n",
    "      first_value(revenue) OVER (\n",
    "        PARTITION BY order_date ORDER BY revenue DESC\n",
    "      ) first_revenue\n",
    "    FROM daily_product_revenue t\n",
    "    ORDER BY order_date, revenue DESC\n",
    "\"\"\").\n",
    "    show(100, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see an example with last_value. While using last_value we need to specify **ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING**.\n",
    "* By default it uses `ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW`.\n",
    "* The last value with in `UNBOUNDED PRECEDING AND CURRENT ROW` will be current record.\n",
    "* To get the right value, we have to change the windowing clause to `ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT t.*,\n",
    "    last_value(order_item_product_id) OVER (\n",
    "        PARTITION BY order_date ORDER BY revenue\n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "    ) last_product_id,\n",
    "    last_value(revenue) OVER (\n",
    "        PARTITION BY order_date ORDER BY revenue\n",
    "    ) last_revenue\n",
    "FROM daily_product_revenue AS t\n",
    "ORDER BY order_date, revenue DESC\n",
    "LIMIT 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT t.*,\n",
    "    last_value(order_item_product_id) OVER (\n",
    "        PARTITION BY order_date ORDER BY revenue\n",
    "        ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING\n",
    "    ) last_product_id,\n",
    "    last_value(revenue) OVER (\n",
    "        PARTITION BY order_date ORDER BY revenue\n",
    "        ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING\n",
    "    ) last_revenue\n",
    "FROM daily_product_revenue AS t\n",
    "ORDER BY order_date, revenue DESC\n",
    "LIMIT 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT t.*,\n",
    "      last_value(order_item_product_id) OVER (\n",
    "        PARTITION BY order_date ORDER BY revenue\n",
    "        ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING\n",
    "      ) last_product_id,\n",
    "      last_value(revenue) OVER (\n",
    "        PARTITION BY order_date ORDER BY revenue\n",
    "        ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING\n",
    "      )  last_revenue\n",
    "    FROM daily_product_revenue AS t\n",
    "    ORDER BY order_date, revenue DESC\n",
    "\"\"\").\n",
    "    show(100, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking using Windowing Functions\n",
    "\n",
    "Let us see how we can assign ranks using different **rank** functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/NsqHuZbUrAw?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/NsqHuZbUrAw?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we have to assign ranks globally, we just need to specify **ORDER BY**\n",
    "* If we have to assign ranks with in a key then we need to specify **PARTITION BY** and then **ORDER BY**.\n",
    "* By default **ORDER BY** will sort the data in ascending order. We can change the order by passing **DESC** after order by.\n",
    "* We have 3 main functions to assign ranks - `rank`, `dense_rank` and `row_number`. We will see the difference between the 3 in a moment.\n",
    "\n",
    "Here is an example to assign sparse ranks using the table daily_product_revenue with in each day based on revenue. We can use `rank` function to assign sparse ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT t.*,\n",
    "  rank() OVER (\n",
    "    PARTITION BY order_date\n",
    "    ORDER BY revenue DESC\n",
    "  ) AS rnk\n",
    "FROM daily_product_revenue t\n",
    "ORDER BY order_date, revenue DESC\n",
    "LIMIT 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Here is another example to assign ranks using employees data set with in each department. We can also use other functions such as `dense_rank` and `row_number` to assign ranks.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT\n",
    "  employee_id,\n",
    "  department_id,\n",
    "  salary,\n",
    "  rank() OVER (\n",
    "    PARTITION BY department_id\n",
    "    ORDER BY salary DESC\n",
    "  ) rnk,\n",
    "  dense_rank() OVER (\n",
    "    PARTITION BY department_id\n",
    "    ORDER BY salary DESC\n",
    "  ) drnk,\n",
    "  row_number() OVER (\n",
    "    PARTITION BY department_id\n",
    "    ORDER BY salary DESC\n",
    "  ) rn\n",
    "FROM employees\n",
    "ORDER BY department_id, salary DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  employee_id,\n",
    "  department_id,\n",
    "  salary,\n",
    "  rank() OVER (\n",
    "    PARTITION BY department_id\n",
    "    ORDER BY salary DESC\n",
    "  ) rnk,\n",
    "  dense_rank() OVER (\n",
    "    PARTITION BY department_id\n",
    "    ORDER BY salary DESC\n",
    "  ) drnk,\n",
    "  row_number() OVER (\n",
    "    PARTITION BY department_id\n",
    "    ORDER BY salary DESC, employee_id\n",
    "  ) rn\n",
    "FROM employees\n",
    "ORDER BY department_id, salary DESC\n",
    "\"\"\").\n",
    "    show(100, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM employees ORDER BY salary LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Here is the example for global rank with out `PARTITION BY` clause.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT employee_id, salary,\n",
    "    dense_rank() OVER (ORDER BY salary DESC) AS drnk\n",
    "FROM employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us understand the difference between **rank**, **dense_rank** and **row_number**.\n",
    "\n",
    "* We can use either of the functions to generate ranks when there are no duplicates in the column based on which ranks are assigned.\n",
    "* When the column based on which ranks are assigned have duplicates then row_number should not be used as it generate unique number for each record with in the partition. For those duplicate values, the row number need not be same across multiple runs.\n",
    "* **rank** will skip the ranks in between if multiple people get the same rank while **dense_rank** will not skip the ranks based up on the number of times the value is repeated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order of execution of SQL\n",
    "\n",
    "Let us review the order of execution of SQL. First let us review the order of writing the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/UNCJNFMyr6c?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/UNCJNFMyr6c?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **SELECT**\n",
    "2. **FROM**\n",
    "3. **JOIN** or **OUTER JOIN** with **ON**\n",
    "4. **WHERE**\n",
    "5. **GROUP BY** and optionally **HAVING**\n",
    "6. **ORDER BY**\n",
    "\n",
    "Let us come up with a query which will compute daily revenue using COMPLETE or CLOSED orders and also sorted by order_date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT o.order_date,\n",
    "  round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date\n",
    "ORDER BY o.order_date\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT o.order_date,\n",
    "    round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date\n",
    "    HAVING round(sum(oi.order_item_subtotal), 2) >= 50000\n",
    "ORDER BY order_date\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However order of execution is typically as follows.\n",
    "\n",
    "1. **FROM**\n",
    "2. **JOIN** or **OUTER JOIN** with **ON**\n",
    "3. **WHERE**\n",
    "4. **GROUP BY** and optionally **HAVING**\n",
    "5. **SELECT**\n",
    "6. **ORDER BY**\n",
    "\n",
    "As **SELECT** is executed before **ORDER BY** clause, we will not be able to refer the aliases defined in **SELECT** caluse in other clauses except for **ORDER BY** in most of the traditional databases. However, in Spark we can specify the aliases defined in **SELECT** in **HAVING** as well as **ORDER BY**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{error}\n",
    "This will fail as revenue which is an alias defined in **SELECT** cannot be used in **WHERE**.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT o.order_date,\n",
    "    round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "    AND revenue >= 50000\n",
    "GROUP BY o.order_date\n",
    "ORDER BY order_date\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "This will work as revenue which is an alias defined in **SELECT** can be used in **HAVING** as well as **ORDER BY**.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT o.order_date,\n",
    "    round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date\n",
    "    HAVING revenue >= 50000\n",
    "ORDER BY order_date,\n",
    "    revenue DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Sub Queries\n",
    "\n",
    "Let us recap about Sub Queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6AOlttTRG48?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6AOlttTRG48?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We typically have Sub Queries in **FROM** Clause.\n",
    "* We need not provide alias to the Sub Queries in **FROM** Clause in Spark SQL. In earlier versions, you might have to provide alias for the Sub Query.\n",
    "* We use Sub Queries quite often over queries using Analytics/Windowing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM (SELECT current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM (SELECT current_date) AS q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see few more examples with respect to Sub Queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM (\n",
    "    SELECT order_date, count(1) AS order_count\n",
    "    FROM orders\n",
    "    GROUP BY order_date\n",
    ") q\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Here is an example of how we can filter based up on the derived columns using sub query. However, this can be achieved with direct query as well using `HAVING`. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM (\n",
    "    SELECT order_date, count(1) AS order_count\n",
    "    FROM orders\n",
    "    GROUP BY order_date\n",
    ") q\n",
    "WHERE q.order_count > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT order_date, count(1) AS order_count\n",
    "FROM orders\n",
    "GROUP BY order_date\n",
    "    HAVING count(1) > 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering - Window Function Results\n",
    "\n",
    "Let us understand how to filter on top of results of Window Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/8ncS4CCdVA0?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/8ncS4CCdVA0?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can use **Window Functions** only in **SELECT** Clause.\n",
    "* If we have to filter based on Window Function results, then we need to use Sub Queries.\n",
    "* Once the query with window functions is defined as sub query, we can apply filter using aliases provided for the Window Functions.\n",
    "\n",
    "Here is the example where we can filter data based on Window Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM (\n",
    "  SELECT t.*,\n",
    "    dense_rank() OVER (\n",
    "      PARTITION BY order_date\n",
    "      ORDER BY revenue DESC\n",
    "    ) AS drnk\n",
    "  FROM daily_product_revenue t\n",
    ") q\n",
    "WHERE q.drnk <= 5\n",
    "ORDER BY q.order_date, q.revenue DESC\n",
    "LIMIT 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM (\n",
    "  SELECT t.*,\n",
    "    dense_rank() OVER (\n",
    "      PARTITION BY order_date\n",
    "      ORDER BY revenue DESC\n",
    "    ) AS drnk\n",
    "  FROM daily_product_revenue t\n",
    ") q\n",
    "WHERE q.drnk <= 5\n",
    "ORDER BY q.order_date, q.revenue DESC\n",
    "\"\"\").\n",
    "    show(100, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking and Filtering - Recap\n",
    "\n",
    "Let us recap the procedure to get top 5 orders by revenue for each day.\n",
    "\n",
    "* We have our original data in **orders** and **order_items**\n",
    "* We can pre-compute the data and store in a table or create a view with the logic to generate **daily product revenue**\n",
    "* Then, we have to use the view or table or even sub query to compute rank\n",
    "* We can use the query with ranks as sub query to filter so that we can get top 5 products by revenue.\n",
    "* Let us see the overall process in action.\n",
    "\n",
    "Let us come up with the query to compute daily product revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DESCRIBE orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DESCRIBE order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT o.order_date,\n",
    "       oi.order_item_product_id,\n",
    "       round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date, oi.order_item_product_id\n",
    "ORDER BY o.order_date, revenue DESC\n",
    "LIMIT 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute the rank for each product with in each date using revenue as criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT q.*,\n",
    "  rank() OVER (\n",
    "    PARTITION BY order_date\n",
    "    ORDER BY revenue DESC\n",
    "  ) AS rnk,\n",
    "  dense_rank() OVER (\n",
    "    PARTITION BY order_date\n",
    "    ORDER BY revenue DESC\n",
    "  ) AS drnk\n",
    "FROM (SELECT o.order_date,\n",
    "        oi.order_item_product_id,\n",
    "        round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "      FROM orders o JOIN order_items oi\n",
    "      ON o.order_id = oi.order_item_order_id\n",
    "      WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "      GROUP BY o.order_date, oi.order_item_product_id) q\n",
    "ORDER BY order_date, revenue DESC\n",
    "LIMIT 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see how we can filter the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM (SELECT q.*,\n",
    "  dense_rank() OVER (\n",
    "    PARTITION BY order_date\n",
    "    ORDER BY revenue DESC\n",
    "  ) AS drnk\n",
    "FROM (SELECT o.order_date,\n",
    "        oi.order_item_product_id,\n",
    "        round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "      FROM orders o JOIN order_items oi\n",
    "      ON o.order_id = oi.order_item_order_id\n",
    "      WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "      GROUP BY o.order_date, oi.order_item_product_id) q) q1\n",
    "WHERE drnk <= 5\n",
    "ORDER BY order_date, revenue DESC\n",
    "LIMIT 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE daily_product_revenue\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM (SELECT dpr.*,\n",
    "  dense_rank() OVER (\n",
    "    PARTITION BY order_date\n",
    "    ORDER BY revenue DESC\n",
    "  ) AS drnk\n",
    "FROM daily_product_revenue AS dpr)\n",
    "WHERE drnk <= 5\n",
    "ORDER BY order_date, revenue DESC\n",
    "LIMIT 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises - Windowing Functions\n",
    "\n",
    "Let us take care of the exercises related to windowing or analytics functions. We will be using HR database for the same.\n",
    "\n",
    "* Get all the employees who is making more than average salary with in each department.\n",
    "* Get cumulative salary for one of the department along with department name.\n",
    "* Get top 3 paid employees with in each department by salary (use dense_rank)\n",
    "* Get top 3 products sold in the month of 2014 January by revenue.\n",
    "* Get top 3 products in each category sold in the month of 2014 January by revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Get all the employees who is making more than average salary with in each department.\n",
    "\n",
    "* Use HR database employees and department tables for this problem.\n",
    "* Compute average salary expense for each department and get those employee details who are making more salary than average salary.\n",
    "* Make sure average salary expense per department is rounded off to 2 decimals.\n",
    "* Output should contain employee_id, department_name, salary and avg_salary_expense (derived field).\n",
    "* Data should be sorted in ascending order by department_id and descending order by salary.\n",
    "\n",
    "|employee_id|department_name|salary|avg_salary_expense|\n",
    "|---|---|---|---|\n",
    "|201|Marketing|13000.00|9500.00|\n",
    "|114|Purchasing|11000.00|4150.00|\n",
    "|121|Shipping|8200.00|3475.56|\n",
    "|120|Shipping|8000.00|3475.56|\n",
    "|122|Shipping|7900.00|3475.56|\n",
    "|123|Shipping|6500.00|3475.56|\n",
    "|124|Shipping|5800.00|3475.56|\n",
    "|184|Shipping|4200.00|3475.56|\n",
    "|185|Shipping|4100.00|3475.56|\n",
    "|192|Shipping|4000.00|3475.56|\n",
    "|193|Shipping|3900.00|3475.56|\n",
    "|188|Shipping|3800.00|3475.56|\n",
    "|137|Shipping|3600.00|3475.56|\n",
    "|189|Shipping|3600.00|3475.56|\n",
    "|141|Shipping|3500.00|3475.56|\n",
    "|103|IT|9000.00|5760.00|\n",
    "|104|IT|6000.00|5760.00|\n",
    "|145|Sales|14000.00|8955.88|\n",
    "|146|Sales|13500.00|8955.88|\n",
    "|147|Sales|12000.00|8955.88|\n",
    "|168|Sales|11500.00|8955.88|\n",
    "|148|Sales|11000.00|8955.88|\n",
    "|174|Sales|11000.00|8955.88|\n",
    "|149|Sales|10500.00|8955.88|\n",
    "|162|Sales|10500.00|8955.88|\n",
    "|156|Sales|10000.00|8955.88|\n",
    "|150|Sales|10000.00|8955.88|\n",
    "|169|Sales|10000.00|8955.88|\n",
    "|170|Sales|9600.00|8955.88|\n",
    "|163|Sales|9500.00|8955.88|\n",
    "|151|Sales|9500.00|8955.88|\n",
    "|157|Sales|9500.00|8955.88|\n",
    "|158|Sales|9000.00|8955.88|\n",
    "|152|Sales|9000.00|8955.88|\n",
    "|100|Executive|24000.00|19333.33|\n",
    "|108|Finance|12000.00|8600.00|\n",
    "|109|Finance|9000.00|8600.00|\n",
    "|205|Accounting|12000.00|10150.00|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Get cumulative salary with in each department for Finance and IT department along with department name.\n",
    "\n",
    "* Use HR database employees and department tables for this problem.\n",
    "* Compute cumulative salary expense for **Finance** as well as **IT** departments with in respective departments.\n",
    "* Make sure cumulative salary expense per department is rounded off to 2 decimals.\n",
    "* Output should contain employee_id, department_name, salary and cum_salary_expense (derived field).\n",
    "* Data should be sorted in ascending order by department_name and then salary.\n",
    "\n",
    "|employee_id|department_name|salary|cum_salary_expense|\n",
    "|---|---|---|---|\n",
    "|113|Finance|6900.00|6900.00|\n",
    "|111|Finance|7700.00|14600.00|\n",
    "|112|Finance|7800.00|22400.00|\n",
    "|110|Finance|8200.00|30600.00|\n",
    "|109|Finance|9000.00|39600.00|\n",
    "|108|Finance|12000.00|51600.00|\n",
    "|107|IT|4200.00|4200.00|\n",
    "|106|IT|4800.00|9000.00|\n",
    "|105|IT|4800.00|13800.00|\n",
    "|104|IT|6000.00|19800.00|\n",
    "|103|IT|9000.00|28800.00|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Get top 3 paid employees with in each department by salary (use dense_rank)\n",
    "\n",
    "* Use HR database employees and department tables for this problem.\n",
    "* Highest paid employee should be ranked first.\n",
    "* Output should contain employee_id, department_id, department_name, salary and employee_rank (derived field).\n",
    "* Data should be sorted in ascending order by department_id in ascending order and then salary in descending order.\n",
    "\n",
    "|employee_id|department_id|department_name|salary|employee_rank|\n",
    "|---|---|---|---|---|\n",
    "|200|10|Administration|4400.00|1|\n",
    "|201|20|Marketing|13000.00|1|\n",
    "|202|20|Marketing|6000.00|2|\n",
    "|114|30|Purchasing|11000.00|1|\n",
    "|115|30|Purchasing|3100.00|2|\n",
    "|116|30|Purchasing|2900.00|3|\n",
    "|203|40|Human Resources|6500.00|1|\n",
    "|121|50|Shipping|8200.00|1|\n",
    "|120|50|Shipping|8000.00|2|\n",
    "|122|50|Shipping|7900.00|3|\n",
    "|103|60|IT|9000.00|1|\n",
    "|104|60|IT|6000.00|2|\n",
    "|105|60|IT|4800.00|3|\n",
    "|106|60|IT|4800.00|3|\n",
    "|204|70|Public Relations|10000.00|1|\n",
    "|145|80|Sales|14000.00|1|\n",
    "|146|80|Sales|13500.00|2|\n",
    "|147|80|Sales|12000.00|3|\n",
    "|100|90|Executive|24000.00|1|\n",
    "|101|90|Executive|17000.00|2|\n",
    "|102|90|Executive|17000.00|2|\n",
    "|108|100|Finance|12000.00|1|\n",
    "|109|100|Finance|9000.00|2|\n",
    "|110|100|Finance|8200.00|3|\n",
    "|205|110|Accounting|12000.00|1|\n",
    "|206|110|Accounting|8300.00|2|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Get top 3 products sold in the month of 2014 January by revenue.\n",
    "\n",
    "* Use retail database tables such as orders, order_items and products.\n",
    "* Highest revenue generating product should come at top.\n",
    "* Output should contain product_id, product_name, revenue, product_rank. **revenue** and **product_rank** are derived fields.\n",
    "* Data should be sorted in descending order by revenue.\n",
    "\n",
    "|product_id|product_name|revenue|product_rank|\n",
    "|---|---|---|---|\n",
    "|1004|Field & Stream Sportsman 16 Gun Fire Safe|250787.46|1|\n",
    "|365|Perfect Fitness Perfect Rip Deck|151474.75|2|\n",
    "|957|Diamondback Women's Serene Classic Comfort Bi|148190.12|3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Get top 3 products sold in the month of 2014 January under selected categories by revenue. The categories are **Cardio Equipment** and **Strength Training**.\n",
    "\n",
    "* Use retail database tables such as orders, order_items, products as well as categories.\n",
    "* Highest revenue generating product should come at top.\n",
    "* Output should contain category_id, category_name, product_id, product_name, revenue, product_rank. revenue and product_rank are derived fields.\n",
    "* Data should be sorted in ascending order by category_id and descending order by revenue.\n",
    "\n",
    "|category_id|category_name|product_id|product_name|revenue|product_rank|\n",
    "|---|---|---|---|---|---|\n",
    "|9|Cardio Equipment|191|Nike Men's Free 5.0+ Running Shoe|132286.77|1|\n",
    "|9|Cardio Equipment|172|Nike Women's Tempo Shorts|870.00|2|\n",
    "|10|Strength Training|208|SOLE E35 Elliptical|1999.99|1|\n",
    "|10|Strength Training|203|GoPro HERO3+ Black Edition Camera|1199.97|2|\n",
    "|10|Strength Training|216|Yakima DoubleDown Ace Hitch Mount 4-Bike Rack|189.00|3|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
