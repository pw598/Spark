{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "Let us get started to get into Spark SQL. In this module we will see how to launch and use Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/p83npp8FYe0?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/p83npp8FYe0?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Overview of Spark Documentation\n",
    "* Launching and Using Spark SQL\n",
    "* Overview of Spark SQL Properties\n",
    "* Running OS Commands using Spark SQL\n",
    "* Understanding Warehouse Directory\n",
    "* Managing Spark Metastore Databases\n",
    "* Managing Spark Metastore Tables\n",
    "* Retrieve Metadata of Tables\n",
    "* Role of Spark or Hive Metastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Spark Documentation\n",
    "\n",
    "Let us go through the details related to Spark Documentation. It is very important for you to get comfortable with Spark Documentation if you are aspiring for open book certification exams like CCA 175."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_gAD1tdxoKw?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_gAD1tdxoKw?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Click [here](https://spark.apache.org/docs/latest/sql-programming-guide.html) to go to latest Spark SQL and Data Frames documentation. \n",
    "* We typically get documentation for latest version.\n",
    "* We can replace **latest** in the URL with the version of Spark to get specific version's official documentation.\n",
    "* Also we have resources provided by **databricks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching and Using Spark SQL\n",
    "\n",
    "Let us understand how to launch Spark SQL CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/R-qwe2E9ob0?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/R-qwe2E9ob0?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logon to the gateway node of the cluster.\n",
    "* We have 2 versions of Spark in our labs. One can use `spark-sql` to launch Spark SQL using 1.6.x and `spark2-sql` to launch Spark SQL using 2.3.x.\n",
    "* Launch Spark SQL CLI using `spark-sql`. In clustered mode we might have to add additional arguments. For example\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "* One can get help using `spark-sql --help`\n",
    "* For e. g.: we can use `spark-sql --database training_retail` to connect to specific database. Here is the example in clustered mode.\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse \\\n",
    "    --database ${USER}_retail\n",
    "```\n",
    "* Spark SQL CLI will be launched and will be connected to **${USER}_retail** database.\n",
    "* We can validate to which database we are connected to using `SELECT current_database()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Spark SQL Properties\n",
    "Let us understand details about Spark SQL properties which control Spark SQL run time environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/2JrWYGbBR_8?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/2JrWYGbBR_8?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spark SQL inherits properties defined for Spark. There are some Spark SQL related properties as well and these are applicable even for Data Frames.\n",
    "* We can review these properties using Management Tools such as **Ambari** or **Cloudera Manager Web UI**\n",
    "* Spark run time behavior is controlled by HDFS Properties files, YARN Properties files, Hive Properties files etc in those clusters where Spark is integrated with Hadoop and Hive.\n",
    "* We can get all the properties using `SET;` in Spark SQL CLI\n",
    "\n",
    "Let us review some important properties in Spark SQL. \n",
    "\n",
    "```\n",
    "spark.sql.warehouse.dir\n",
    "spark.sql.catalogImplementation\n",
    "```\n",
    "* We can review the current value using `SET spark.sql.warehouse.dir;`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    config(\"spark.sql.warehouse.dir\", s\"/user/${username}/warehouse\").\n",
    "    enableHiveSupport.\n",
    "    master(\"yarn\").\n",
    "    appName(s\"${username} | Spark SQL - Getting Started\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "+--------------------+--------------------+\n",
       "|                 key|               value|\n",
       "+--------------------+--------------------+\n",
       "|spark.sql.warehou...|/user/itversity/w...|\n",
       "+--------------------+--------------------+\n",
       "\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SET spark.sql.warehouse.dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Properties with default values does not show up as part of `SET` command. But we can check and overwrite the values - for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SET spark.sql.shuffle.partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can overwrite property by setting value using the same **SET** command, eg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SET spark.sql.shuffle.partitions=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running OS Commands using Spark SQL\n",
    "\n",
    "Let us understand how to run OS commands using Spark SQL CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/1yp8aTJXYAI?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/1yp8aTJXYAI?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can run OS commands using **!** at the beginning.\n",
    "  * Listing local Files `!ls -ltr;`\n",
    "  * Listing HDFS Files `!hdfs dfs -ls /public/retail_db;`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "\"ls -ltr\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "\"hdfs dfs -ls /public/retail_db\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Warehouse Directory\n",
    "\n",
    "Let us go through the details related to Spark Metastore Warehouse Directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/JjMnOstLLUk?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/JjMnOstLLUk?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A Database in Spark SQL is nothing but directory in underlying file system like HDFS. \n",
    "* A Spark Metastore Table is nothing but directory in underlying file systems like HDFS.\n",
    "* A Partition of Spark Metastore Table is nothing but directory in underlying file systems like HDFS under table.\n",
    "* Warehouse Directory is the base directory where directories related to databases, tables go by default.\n",
    "* It is controlled by `spark.sql.warehouse.dir`. You can get the value by saying `SET spark.sql.warehouse.dir;`\n",
    "> <span style=\"color:red\">Do not overwrite this property Spark SQL CLI. It will not have any effect.</span>\n",
    "* Underlying directory for a database will have **.db** extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SET spark.sql.warehouse.dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Spark Metastore Databases\n",
    "\n",
    "Let us undestand how to manage Spark Metastore Databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/LhayXSutoHk?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/LhayXSutoHk?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make a habit of reviewing Language Manual.\n",
    "* We can create database using **CREATE DATABASE** Command.\n",
    "* For e. g.: `CREATE DATABASE itversity_demo;`\n",
    "* If the database exists it will fail. If you want to ignore with out throwing error you can use **IF NOT EXISTS**\n",
    "* e. g.: `CREATE DATABASE IF NOT EXISTS itversity_demo;`\n",
    "* We can list the databases using `SHOW databases;`\n",
    "* Spark Metastore is multi tenant database. To switch to a database, you can use **USE** Command. e. g.: `USE itversity_demo;`\n",
    "* We can drop empty database by using `DROP DATABASE itversity_demo;`.\n",
    "* Add cascade to drop all the tables and then the database `DROP DATABASE itversity_demo CASCADE;`.\n",
    "* We can also specify location while creating the database - `CREATE DATABASE itversity_demo LOCATION '/user/itversity/custom/itversity_demo.db'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DROP DATABASE IF EXISTS itversity_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE DATABASE itversity_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE DATABASE itversity_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS itversity_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SHOW databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT current_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT current_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val username = System.getProperty(\"user.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "s\"hdfs dfs -ls /user/${username}/warehouse/${username}_demo.db\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE table_demo (i INT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "s\"hdfs dfs -ls /user/${username}/warehouse/${username}_demo.db\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DROP DATABASE itversity_demo CASCADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "s\"hdfs dfs -ls /user/${username}/warehouse/${username}_demo.db\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE DATABASE itversity_demo LOCATION '/user/itversity/custom/itversity_demo.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "s\"hdfs dfs -ls /user/${username}/custom\"! // Directory will be created if it does not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "s\"hdfs dfs -ls /user/${username}/custom/${username}_demo.db\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Spark Metastore Tables\n",
    "\n",
    "Let us create our first Spark Metastore table. We will also have a look into how to list the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/I8-TFwtIHrw?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/I8-TFwtIHrw?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will get into details related to DDL Commands at a later point in time. \n",
    "* For now we will just create our first table. We will get into the details about creating tables as part of subsequent sections.\n",
    "> <span style=\"color:red\">Use your OS username as prefix for the databases, if you are using our labs</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT current_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DROP DATABASE itversity_retail CASCADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE DATABASE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE orders (\n",
    "  order_id INT,\n",
    "  order_date STRING,\n",
    "  order_customer_id INT,\n",
    "  order_status STRING\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ','"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can list the tables using `SHOW tables;`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SHOW tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also drop the table using `DROP TABLE` command. We will get into more details at a later point in time.\n",
    "* We can also truncate the managed tables using `TRUNCATE TABLE` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Metadata of Tables\n",
    "\n",
    "As the table is created, let us understand how to get the metadata of a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AHaoOnQci30?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AHaoOnQci30?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can get metadata of Hive Tables using several commands.\n",
    "  * DESCRIBE - e.g.: `DESCRIBE orders;`\n",
    "  * DESCRIBE EXTENDED - e.g.: `DESCRIBE EXTENDED orders;`\n",
    "  * DESCRIBE FORMATTED - e.g.: `DESCRIBE FORMATTED orders;`\n",
    "* **DESCRIBE** will give only field names and data types.\n",
    "* **DESCRIBE EXTENDED** will give all the metadata, but not in readable format in Hive. It is same as **DESCRIBE FORMATTED** in Spark SQL.\n",
    "* **DESCRIBE FORMATTED** will give metadata in readable format.\n",
    "\n",
    "**As the output is truncated using Jupyter, we will actually see the details using `spark-sql`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT current_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SHOW tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DESCRIBE orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DESCRIBE EXTENDED orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DESCRIBE FORMATTED orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role of Spark or Hive Metastore\n",
    "\n",
    "Let us understand the role of Spark Metastore or Hive Metasore. We need to first understand details related to Metadata generated for Spark Metastore tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MKzHiKgKHIY?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MKzHiKgKHIY?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When we create a Spark Metastore table, there is metadata associated with it.\n",
    "  * Table Name\n",
    "  * Column Names and Data Types\n",
    "  * Location\n",
    "  * File Format\n",
    "  * and more\n",
    "* This metadata has to be stored some where so that Query Engines such as Spark SQL can access the information to serve our queries.\n",
    "\n",
    "Let us understand where the metadata is stored.\n",
    "\n",
    "* Information is typically stored in relational database and it is called as metastore.\n",
    "* It is extensively used by Hive or Spark SQL engine for syntax and semantics check as well as execution of queries.\n",
    "* In our case it is stored in MySQL Database. Let us review the details by going through relevant properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Getting Started with Spark SQL\n",
    "\n",
    "Let's do a simple exercise to conclude this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9Wr1A2Mvtv8?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9Wr1A2Mvtv8?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Launch Spark SQL (don't use database) (use spark-sql command). Here is the script for our labs. In other environments, you can skip last line. I have also included commands to launch Spark using Scala or Python (for CCA 175 Certification purpose)\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "* Create Database and exit (make sure to prefix database name with your OS username, e. g.: **training_retail** for OS user training)\n",
    "* Exit and launch connecting to your database\n",
    "* Create Table orders. You can use below script to create the table.\n",
    "\n",
    "```\n",
    "CREATE TABLE orders (\n",
    "  order_id INT,\n",
    "  order_date STRING,\n",
    "  order_customer_id INT,\n",
    "  order_status STRING\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n",
    "```\n",
    "\n",
    "* List the tables\n",
    "* Describe the table and review the whole metadata"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
