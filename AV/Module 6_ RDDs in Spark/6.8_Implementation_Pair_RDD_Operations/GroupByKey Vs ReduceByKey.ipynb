{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><h1> GroupByKey Vs ReduceByKey </h1></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the spark context\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark context object\n",
    "sc = SparkContext(appName=\"pair_rdd_operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "We have a students data in the file **students_data.txt** which has the 9 different columns. First one is **Roll No**, **Section**, **Name of Student**, **City**, and the last five columns are marks in 5 different subjects. The data of each student is in different row separated by space.\n",
    "\n",
    "---\n",
    "\n",
    "![](images/data_1.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### `Create the rdd of the file - students_data.txt`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rdd of the file\n",
    "students_marks = sc.textFile(\"data/students_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['101 A Rohit Gurgaon 65 77 43 66 87',\n",
       " '102 B Akansha Delhi 55 46 24 66 77',\n",
       " '103 A Himanshu Faridabad 75 38 84 38 58',\n",
       " '104 A Ekta Delhi 85 84 39 58 85',\n",
       " '105 B Deepanshu Gurgaon 34 55 56 23 66',\n",
       " '106 B Ayush Delhi 66 62 98 74 87',\n",
       " '107 B Aditi Delhi 76 83 75 38 58',\n",
       " '108 A Sahil Faridabad 55 32 43 56 66',\n",
       " '109 A Krati Delhi 34 53 25 67 75']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## view the data\n",
    "students_marks.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next, We will split each line into a list of words using Map. Let's see how to do this with the help of map transformation in the below cell. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the rdd\n",
    "students_marks = students_marks.map(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['101', 'A', 'Rohit', 'Gurgaon', '65', '77', '43', '66', '87'],\n",
       " ['102', 'B', 'Akansha', 'Delhi', '55', '46', '24', '66', '77'],\n",
       " ['103', 'A', 'Himanshu', 'Faridabad', '75', '38', '84', '38', '58'],\n",
       " ['104', 'A', 'Ekta', 'Delhi', '85', '84', '39', '58', '85'],\n",
       " ['105', 'B', 'Deepanshu', 'Gurgaon', '34', '55', '56', '23', '66'],\n",
       " ['106', 'B', 'Ayush', 'Delhi', '66', '62', '98', '74', '87'],\n",
       " ['107', 'B', 'Aditi', 'Delhi', '76', '83', '75', '38', '58'],\n",
       " ['108', 'A', 'Sahil', 'Faridabad', '55', '32', '43', '56', '66'],\n",
       " ['109', 'A', 'Krati', 'Delhi', '34', '53', '25', '67', '75']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect the rdd\n",
    "students_marks.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Find out the number of students in each section?`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Now, we will create another paired RDD where key will be the section of the student and marks of students as the values.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pair rdd with section of students as key\n",
    "students_section = students_marks.map(lambda x: (x[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 1),\n",
       " ('B', 1),\n",
       " ('A', 1),\n",
       " ('A', 1),\n",
       " ('B', 1),\n",
       " ('B', 1),\n",
       " ('B', 1),\n",
       " ('A', 1),\n",
       " ('A', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect the rdd\n",
    "students_section.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `TRANSFORMATION - GROUPBYKEY`\n",
    "\n",
    "---\n",
    "\n",
    "It receives key-value pairs (K, V) as an input, group the values based on key and generates a dataset of (K, Iterable) pairs as an output.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "![](images/group.png)\n",
    "\n",
    "---\n",
    "\n",
    "MapValues is applicable only for pair RDDs. As its name indicates, this transformation only operates on the values of the pair RDDs instead of operating on the whole tuple.\n",
    "\n",
    "More about MapValues: \n",
    "\n",
    " - http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=mapvalues#pyspark.RDD.mapValues\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by key transformation\n",
    "grouped_students_marks = students_section.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 5), ('B', 4)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect the pair rdd\n",
    "grouped_students_marks.mapValues(sum).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### `TRANSFORMATION -  REDUCEBYKEY`\n",
    "\n",
    "---\n",
    "\n",
    "It uses associative reduce function, where it merges value of each key. It can be used with Rdd only in key value pair.  It merges data locally using associative function for optimized data shuffling. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](images/reduce.png)\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce by key transformation\n",
    "reduced_student_marks = students_section.reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "The below image shows how the reduce function works:\n",
    "\n",
    "---\n",
    "\n",
    "![](images/reduce.svg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 5), ('B', 4)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect the pair rdd\n",
    "reduced_student_marks.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Comparison between GroupByKey and ReduceByKey`\n",
    "\n",
    "---\n",
    "\n",
    "We will create a sample data of 20 million data points and find out the sum of each key using both `groupByKey` and `reduceByKey`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the random library\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list\n",
    "my_section_list = []\n",
    "\n",
    "# loop to create 20 million points\n",
    "for i in range(20000000):\n",
    "    my_section_list.append((random.choice([\"A\",\"B\",\"C\",\"D\",\"E\"]) , random.randint(1,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, the above list will look something like this.\n",
    "\n",
    "---\n",
    "\n",
    " [ (\"A\": 1),\n",
    " \n",
    "   (\"B\": 2),\n",
    " \n",
    "   (\"C\": 1),\n",
    "   \n",
    "   .\n",
    "   \n",
    "   .\n",
    "   \n",
    "   .\n",
    "   \n",
    "   (\"A\": 4)\n",
    "]\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "**Create RDD of the collection**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rdd of the collection\n",
    "rdd = sc.parallelize(my_section_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `GroupByKey`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the rdd\n",
    "rdd_grouped = rdd.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', 21994363),\n",
       " ('E', 22017465),\n",
       " ('D', 21988704),\n",
       " ('C', 21990282),\n",
       " ('A', 22029015)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect the results\n",
    "rdd_grouped.mapValues(sum).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "![](images/groupByKey.png)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In the above image, we can see that total 27 MB of data was shuffled across the partitions. Now, let's do the same task using the `reduceByKey` and see if there is any difference in the performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `ReduceByKey`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the rdd\n",
    "rdd_reduced = rdd.reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', 21994363),\n",
       " ('E', 22017465),\n",
       " ('D', 21988704),\n",
       " ('C', 21990282),\n",
       " ('A', 22029015)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect the results\n",
    "rdd_reduced.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "![](images/reduceByKey.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Using `reduceByKey`, only 2.3 KB of data was shuffled. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
