{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<center> <h1> Transformations & Actions on RDDs </h1> </center>\n",
    "\n",
    "In the notebook, we will implement different transformations and actions using Python.\n",
    "\n",
    "\n",
    "\n",
    "### `Transformations` \n",
    "\n",
    "\n",
    "We will do the following transformations in this notebook:\n",
    "\n",
    "* 1. **Map**\n",
    "* 2. **FlatMap**\n",
    "* 3. **Filter**\n",
    "* 4. **Distinct**\n",
    "* 5. **Union**\n",
    "* 6. **Intersection**\n",
    "\n",
    "\n",
    "### `Actions` \n",
    "\n",
    "\n",
    "We will do the following Actions in this notebook:\n",
    "\n",
    "* 1. **Collect**\n",
    "* 2. **Take**\n",
    "* 3. **Count**\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### `IMPORTING THE REQUIRED LIBRARIES`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://gw02.itversity.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.2.6.5.0-292</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "#### ` Problem Statement`\n",
    "\n",
    "Suppose there is an orgranization name **`Analytics 20`**. It has 2 different branches, one in **`India`** and another one is in **`Dubai`** We have generated a random data of the employees of this organization. One file **`analytics_20_india.txt`** contains the data of employees of India and another one **`analytics_20_dubai.txt`** that contains the data of employees of Dubai. \n",
    "\n",
    "Each line of the both the files contains 3 columns. First one is `Name of the employee`, second one is `Department Name` in which he/she works and last one is `Place Name` to which the employee belongs. Data is as shown below - \n",
    "\n",
    "<center><img src=\"images/rdd_op_dataset.png\"></center>\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "#### `Reading the file - analytics_20_india.txt`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "analytics_india = sc.textFile(\"data/analytics_20_india.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the type of the file\n",
    "type(analytics_india) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Once we read the file in the spark, it has been converted into an RDD. \n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "#### `Action - collect` \n",
    "\n",
    "**collect** action will return the complete output.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Keaton Data_Science India',\n",
       " 'Idona Data_Science Australia',\n",
       " 'Janna HR India',\n",
       " 'Damon Data_Science India',\n",
       " 'Rahim Marketing India',\n",
       " 'Audrey Data_Science India',\n",
       " 'Irma HR Dubai',\n",
       " 'Tatum HR India',\n",
       " 'Acton Data_Science India',\n",
       " 'Ainsley Data_Science India',\n",
       " 'Phillip Data_Science India',\n",
       " 'Maite Marketing India',\n",
       " 'Kevyn Marketing Australia',\n",
       " 'Vielka HR India',\n",
       " 'Risa Operations India',\n",
       " 'Jael Accounts Dubai',\n",
       " 'Erich Data_Science India',\n",
       " 'Pearl Operations Australia',\n",
       " 'Francesca Data_Science India',\n",
       " 'Ross Sales India',\n",
       " 'Tarik HR Dubai',\n",
       " 'Lev HR India',\n",
       " 'Nerea Accounts India',\n",
       " 'Halla Sales India',\n",
       " 'Daquan Legal India',\n",
       " 'Ivan HR India',\n",
       " 'Venus HR India',\n",
       " 'Lareina Legal India',\n",
       " 'Orlando Sales Australia',\n",
       " 'Denise Accounts India',\n",
       " 'Alvin Accounts Dubai',\n",
       " 'Rafael Data_Science Australia',\n",
       " 'Whoopi Data_Science Australia',\n",
       " 'Norman Legal Dubai',\n",
       " 'Forrest Sales Dubai',\n",
       " 'Sigourney Legal India',\n",
       " 'Stone Legal Scotland',\n",
       " 'Todd Sales India',\n",
       " 'Jerome Sales India',\n",
       " 'Signe HR India',\n",
       " 'Xavier Legal India',\n",
       " 'Kevin Customer_Support India',\n",
       " 'Michelle Customer_Support India',\n",
       " 'Lyle Customer_Support Dubai',\n",
       " 'Brendan Data_Science Australia',\n",
       " 'Melvin Data_Science Australia',\n",
       " 'Ignacia Customer_Support India',\n",
       " 'Lenore HR India']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect all records\n",
    "analytics_india.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "#### `Action - take`\n",
    "\n",
    "**take** action will return the top n (takes an integer as a parameter) results of the query. It the similar to the head funciton of pandas.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Keaton Data_Science India',\n",
       " 'Idona Data_Science Australia',\n",
       " 'Janna HR India',\n",
       " 'Damon Data_Science India',\n",
       " 'Rahim Marketing India']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take 5 records\n",
    "analytics_india.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "#### `Transformation - map`\n",
    "\n",
    "**map** transformation does the same operation on each of the object. \n",
    "\n",
    "We will split each line into a list of words using **map**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Keaton Data_Science India',\n",
       " 'Idona Data_Science Australia',\n",
       " 'Janna HR India',\n",
       " 'Damon Data_Science India',\n",
       " 'Rahim Marketing India']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original data\n",
    "analytics_india.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map transformation - tokenize records\n",
    "analytics_india_map = analytics_india.map(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Keaton', 'Data_Science', 'India'],\n",
       " ['Idona', 'Data_Science', 'Australia'],\n",
       " ['Janna', 'HR', 'India'],\n",
       " ['Damon', 'Data_Science', 'India'],\n",
       " ['Rahim', 'Marketing', 'India']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check top 5 results\n",
    "analytics_india_map.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "#### `Transformation - distinct`\n",
    "\n",
    "**distinct** is used to find the unique elements in the RDD.\n",
    "\n",
    "Find out the list of unique places of origin of the employees in the India branch.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Keaton', 'Data_Science', 'India'],\n",
       " ['Idona', 'Data_Science', 'Australia'],\n",
       " ['Janna', 'HR', 'India'],\n",
       " ['Damon', 'Data_Science', 'India'],\n",
       " ['Rahim', 'Marketing', 'India']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original map data\n",
    "analytics_india_map.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rdd with only country value\n",
    "analytics_india_places = analytics_india_map.map(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the distinct transformation\n",
    "analytics_india_distinct_places = analytics_india_places.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India', 'Australia', 'Dubai', 'Scotland']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct country of people working in India branch\n",
    "analytics_india_distinct_places.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use count action to find out the total places\n",
    "analytics_india_distinct_places.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `Transformation - filter`\n",
    "\n",
    "**filter** transformation only returns the elements which satisfies the given condition. \n",
    "\n",
    "Find out the data of the people who belong to the **India**.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Keaton', 'Data_Science', 'India'],\n",
       " ['Idona', 'Data_Science', 'Australia'],\n",
       " ['Janna', 'HR', 'India'],\n",
       " ['Damon', 'Data_Science', 'India'],\n",
       " ['Rahim', 'Marketing', 'India']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original map data\n",
    "analytics_india_map.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the people who belong to the India\n",
    "analytics_india_employee_india = analytics_india_map.filter(lambda x: x[2] == \"India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Keaton', 'Data_Science', 'India'],\n",
       " ['Janna', 'HR', 'India'],\n",
       " ['Damon', 'Data_Science', 'India'],\n",
       " ['Rahim', 'Marketing', 'India'],\n",
       " ['Audrey', 'Data_Science', 'India'],\n",
       " ['Tatum', 'HR', 'India'],\n",
       " ['Acton', 'Data_Science', 'India'],\n",
       " ['Ainsley', 'Data_Science', 'India'],\n",
       " ['Phillip', 'Data_Science', 'India'],\n",
       " ['Maite', 'Marketing', 'India'],\n",
       " ['Vielka', 'HR', 'India'],\n",
       " ['Risa', 'Operations', 'India'],\n",
       " ['Erich', 'Data_Science', 'India'],\n",
       " ['Francesca', 'Data_Science', 'India'],\n",
       " ['Ross', 'Sales', 'India'],\n",
       " ['Lev', 'HR', 'India'],\n",
       " ['Nerea', 'Accounts', 'India'],\n",
       " ['Halla', 'Sales', 'India'],\n",
       " ['Daquan', 'Legal', 'India'],\n",
       " ['Ivan', 'HR', 'India'],\n",
       " ['Venus', 'HR', 'India'],\n",
       " ['Lareina', 'Legal', 'India'],\n",
       " ['Denise', 'Accounts', 'India'],\n",
       " ['Sigourney', 'Legal', 'India'],\n",
       " ['Todd', 'Sales', 'India'],\n",
       " ['Jerome', 'Sales', 'India'],\n",
       " ['Signe', 'HR', 'India'],\n",
       " ['Xavier', 'Legal', 'India'],\n",
       " ['Kevin', 'Customer_Support', 'India'],\n",
       " ['Michelle', 'Customer_Support', 'India'],\n",
       " ['Ignacia', 'Customer_Support', 'India'],\n",
       " ['Lenore', 'HR', 'India']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect result\n",
    "analytics_india_employee_india.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's find out the data of the people who belongs to **Dubai** and are in **HR** department.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter transformation\n",
    "analytics_india_filtered_data = analytics_india_map.filter(lambda x: (x[1] == \"HR\") & (x[2] == \"Dubai\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Irma', 'HR', 'Dubai'], ['Tarik', 'HR', 'Dubai']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect the results\n",
    "analytics_india_filtered_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `Transformation - flatmap`\n",
    "\n",
    "* We saw **map** function does a one-to-one transformation. \n",
    "> * It transforms each element of a collection into one element of the resulting collection. \n",
    "\n",
    "<center><img src =\"images/map_transformation.png\"></center>\n",
    "\n",
    "* In the **flatmap** transformation, we will see that instead of multiple lists of each line it will return a single list of output. \n",
    "> * Spark **flatMap** function expresses a one-to-many transformation.\n",
    "\n",
    "Let's see the difference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Keaton Data_Science India',\n",
       " 'Idona Data_Science Australia',\n",
       " 'Janna HR India',\n",
       " 'Damon Data_Science India',\n",
       " 'Rahim Marketing India']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original data\n",
    "analytics_india.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatmap tranformation - tokenize records\n",
    "analytics_india_flatmap = analytics_india.flatMap(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Keaton',\n",
       " 'Data_Science',\n",
       " 'India',\n",
       " 'Idona',\n",
       " 'Data_Science',\n",
       " 'Australia',\n",
       " 'Janna',\n",
       " 'HR',\n",
       " 'India',\n",
       " 'Damon']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatmap tokenize\n",
    "analytics_india_flatmap.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Keaton', 'Data_Science', 'India'],\n",
       " ['Idona', 'Data_Science', 'Australia'],\n",
       " ['Janna', 'HR', 'India'],\n",
       " ['Damon', 'Data_Science', 'India'],\n",
       " ['Rahim', 'Marketing', 'India'],\n",
       " ['Audrey', 'Data_Science', 'India'],\n",
       " ['Irma', 'HR', 'Dubai'],\n",
       " ['Tatum', 'HR', 'India'],\n",
       " ['Acton', 'Data_Science', 'India'],\n",
       " ['Ainsley', 'Data_Science', 'India']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map tokenize\n",
    "analytics_india_map.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `Transformation - union`\n",
    "\n",
    "Use **union** transformation to find out all the places of origin from both branches - India and Dubai.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rdd for Dubai branch file\n",
    "analytics_dubai = sc.textFile(\"data/analytics_20_dubai.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Leo Customer_Support Scotland',\n",
       " 'Cyrus Customer_Support India',\n",
       " 'Jolie Sales India',\n",
       " 'Susan HR Australia',\n",
       " 'Azalia Customer_Support Dubai']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dubai data\n",
    "analytics_dubai.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map tranformation - tokenize records\n",
    "analytics_dubai_places = analytics_dubai.map(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Leo', 'Customer_Support', 'Scotland'],\n",
       " ['Cyrus', 'Customer_Support', 'India'],\n",
       " ['Jolie', 'Sales', 'India'],\n",
       " ['Susan', 'HR', 'Australia'],\n",
       " ['Azalia', 'Customer_Support', 'Dubai']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dubai tokenized records\n",
    "analytics_dubai_places.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select distinct places from Dubai branch\n",
    "\n",
    "# Select country from each record\n",
    "analytics_dubai_places = analytics_dubai_places.map(lambda x: x[2])\n",
    "\n",
    "# Get distinct places\n",
    "analytics_dubai_distinct_places = analytics_dubai_places.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scotland', 'India', 'Australia', 'Dubai', 'South_Africa']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique places from Dubai branch\n",
    "analytics_dubai_distinct_places.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India', 'Australia', 'Dubai', 'Scotland']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique places from India  branch\n",
    "analytics_india_distinct_places.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union places from two branches\n",
    "union_places = analytics_india_distinct_places.union(analytics_dubai_distinct_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India',\n",
       " 'Australia',\n",
       " 'Dubai',\n",
       " 'Scotland',\n",
       " 'Scotland',\n",
       " 'India',\n",
       " 'Australia',\n",
       " 'Dubai',\n",
       " 'South_Africa']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_places.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `Transformation - intersection`\n",
    "\n",
    "Use **intersection** transformation to find out the common places of origin of the employees from both branches - India and Dubai. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itersection of both RDDs of unique places\n",
    "common_places = analytics_india_distinct_places.intersection(analytics_dubai_distinct_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dubai', 'India', 'Australia', 'Scotland']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect the results\n",
    "common_places.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
