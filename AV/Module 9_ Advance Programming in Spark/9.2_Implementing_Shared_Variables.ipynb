{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<center> <h1> Shared Variables: Broadcast </h1> </center>\n",
    "\n",
    "---\n",
    "\n",
    "* **Read-only variable cached on each node rather than shipping a copy of it with tasks.**\n",
    "* **They give every node a copy of a large input dataset in an efficient manner.**\n",
    "* **Attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.**\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Creating the Spark Session`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3fec717-e6a6-43a0-a648-b808fb9f4a3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session object\n",
    "spark = SparkSession.builder.appName(\"shared-Variables-example\").master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://gw02.itversity.com:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.2.6.5.0-292</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>shared-Variables-example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x6946f3f87e10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Creating the sample data and RDD of it`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5a14e329-f236-454b-b5ac-002dfea3fcb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample input data\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "        (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "        (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "        (\"Maria\",\"Jones\",\"USA\",\"FL\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "75958414-5e54-4add-a503-7c62509fe6bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parallelize input data into an RDD\n",
    "rdd_0 = spark.sparkContext.parallelize(data, numSlices= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of partitions\n",
    "rdd_0.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Create the mappings to be broadcasted`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bd99393c-0730-42fc-95bb-9a1379d0582f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mappings to be broadcasted\n",
    "states = {\"NY\":\"New York\",\"CA\":\"California\",\"FL\":\"Florida\", \"NZ\" : \"New Zealand\", \"IND\" :\"India\"}\n",
    "countries = {\"USA\":\"United States of America\",\"IN\":\"India\",\"SA\":\"South Africa\",\"NZ\":\"New Zealand\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Broadcasting the variables`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "21184203-9f54-4a3b-8055-c6b17c33057c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Broadcast variables\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "broadcastCountries = spark.sparkContext.broadcast(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Map the countries & states using the broadcasted variable.`\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a303008f-c0b1-423b-9a93-f0eb1db01693",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use broadcast variable\n",
    "rdd_1 = rdd_0.map(lambda f: (f[0], f[1], broadcastCountries.value[f[2]], broadcastStates.value[f[3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Collect the Results`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "458f7d23-2338-45c5-bc87-112c20e62d47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James', 'Smith', 'United States of America', 'California'),\n",
       " ('Michael', 'Rose', 'United States of America', 'New York'),\n",
       " ('Robert', 'Williams', 'United States of America', 'California'),\n",
       " ('Maria', 'Jones', 'United States of America', 'Florida')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect\n",
    "rdd_1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "\n",
    "\n",
    "<center> <h1> Shared Variables: Accumulators </h1> </center> \n",
    "\n",
    "---\n",
    "\n",
    " * **They are only \"added\" to through an associative and commutative operation.**\n",
    " * **They can be used to implement counters or sums.** \n",
    " * **Spark natively supports accumulators of numeric types, and programmers can add support for new types**\n",
    "\n",
    "\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Initialize the accumulator & create a sample RDD`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the accumulator\n",
    "accuSum=spark.sparkContext.accumulator(0)\n",
    "\n",
    "# create a RDD of empty list\n",
    "rdd=spark.sparkContext.parallelize([1,2,3,4,5], numSlices=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Define the function`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the commutative function\n",
    "def countFun(x):\n",
    "    global accuSum\n",
    "    accuSum += x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Get the sum`\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sum of accumulator\n",
    "rdd.foreach(countFun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the value\n",
    "accuSum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Let's try to find the sum without using accumulator.`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the accumulator\n",
    "accuSum2 = 0  # spark.sparkContext.accumulator(0)\n",
    "\n",
    "# create a RDD of empty list\n",
    "rdd2 = spark.sparkContext.parallelize([1,2,3,4,5], numSlices= 8)\n",
    "\n",
    "# define the commutative function\n",
    "def countFun2(x):\n",
    "    global accuSum\n",
    "    accuSum += x\n",
    "    \n",
    "# get the sum of accumulator\n",
    "rdd2.foreach(countFun2)\n",
    "\n",
    "# get the value\n",
    "accuSum2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Does an empty partition will be used to compute the results of accumulator variable?\n",
    "\n",
    "-  First of all, we will intialize the accumulator variable with 0.\n",
    "-  We will create a list of numbers and create rdd of it.\n",
    "-  Check the number of paritions.\n",
    "-  Check for the empty partitions.  \n",
    "-  Add one to each partition and get the result.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement a counter\n",
    "accumCount=spark.sparkContext.accumulator(0) \n",
    "\n",
    "# create a list of numbers and create its RDD\n",
    "rdd2 = spark.sparkContext.parallelize([1, 2, 3, 4, 5], numSlices=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of partitions\n",
    "rdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, we have just 5 numbers in the list and the rdd is created with number of paritions 8. So, 3 of the 8 partitions must be empty. Let's verify it with the help of the `glom` function.\n",
    "\n",
    "---\n",
    "\n",
    "#### `Check for the empty partitions`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [1], [], [2], [3], [], [4], [5]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for the empty partitions\n",
    "rdd2.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, we will add 1 to each partition value and get the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add 1 to each value\n",
    "rdd2.foreach(lambda x:accumCount.add(1))\n",
    "\n",
    "# final result\n",
    "accumCount.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So, we got the value 5, therefore the empty partitions will not be used to compute the results of accumulator variable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
