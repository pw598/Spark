{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coalesce vs Repartition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### `Load required libraries`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "748391b4-0eb1-45f7-85cb-1cfd955240b5",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import date, timedelta\n",
    "from pyspark.sql.types import IntegerType, DateType, StringType, StructType, StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### `Spark configurations`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark configuration\n",
    "appName = \"PySpark Partition Example\"\n",
    "master = \"local[8]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### `SparkSession object`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with Hive supported.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### `Create sample data`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate sample data\n",
    "start_date = date(2019, 1, 1)\n",
    "\n",
    "data = []\n",
    "\n",
    "# 2 million records\n",
    "for i in range(0, 1000000):\n",
    "    data.append({\"Country\": \"CN\", \"Date\": start_date +\n",
    "                 timedelta(days=i), \"Amount\": 10+i})\n",
    "    data.append({\"Country\": \"AU\", \"Date\": start_date +\n",
    "                 timedelta(days=i), \"Amount\": 10+i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for sample data\n",
    "schema = StructType([StructField('Country', StringType(), nullable=False),\n",
    "                     StructField('Date', DateType(), nullable=False),\n",
    "                     StructField('Amount', IntegerType(), nullable=False)])\n",
    "\n",
    "# Create dataframe\n",
    "df = spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Country='CN', Date=datetime.date(2019, 1, 1), Amount=10),\n",
       " Row(Country='AU', Date=datetime.date(2019, 1, 1), Amount=10),\n",
       " Row(Country='CN', Date=datetime.date(2019, 1, 2), Amount=11),\n",
       " Row(Country='AU', Date=datetime.date(2019, 1, 2), Amount=11),\n",
       " Row(Country='CN', Date=datetime.date(2019, 1, 3), Amount=12)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display dataframe\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of partitions\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### `Repartitioning Data`\n",
    "\n",
    "---\n",
    "\n",
    "There are two functions you can use in Spark to repartition data:\n",
    "\n",
    "`1. coalesce`\n",
    "\n",
    "`2. repartition`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Repartition with Coalesce`\n",
    "\n",
    "---\n",
    "\n",
    "When `coalesce` is defined on an RDD, this operation results in a `Narrow dependency`. For example, if you go from 1000 partitions to 100 partitions, there will no shuffle. Instead, each of the 100 new partitions will claim 10 of the current partitions. However, if a larger number of partitions are requested, it will no happen as `coalesce` does not increase partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Increase Partitions with Coalesce`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get original number of partitions\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase to 16 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "09c2661d-2cd7-4f2a-b417-3f97a2d8acb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Coalesce\n",
    "df = df.coalesce(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of partitions remains same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of partitions\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Decrease Partitions with Coalesce`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decrease to 4 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get original number of partitions\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "408b2219-c52f-421c-a2d1-a31a04ce2143",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Coalesce\n",
    "df = df.coalesce(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of partitions\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "11dfb6fa-55b2-4d17-b2d9-545f8c752fdc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "#### `Repartition with Repartition`\n",
    "\n",
    "---\n",
    "\n",
    "The other method for repartitioning is `Repartition`. It’s defined as the follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3ffef643-40d2-423a-9481-1d6c9d05d179",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### # Defining repartition\n",
    "`def repartition(numPartitions, *cols)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a new `DataFrame` partitioned by the given partitioning expressions. The resulting DataFrame is `hash partitioned`.\n",
    "\n",
    "`numPartitions` can be an int to specify the target number of partitions or it could also be a Column.  If it is a Column, then the data will be partitioned based on the column. If not specified, the default number of partitions is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Increase Partitions with Repartition`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get original number of partitions\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "33ef40ca-7c58-498b-b574-dd32d19571fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartition\n",
    "df = df.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of partitions\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Decrease Partitions with Repartition`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get original number of partitions\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "33ef40ca-7c58-498b-b574-dd32d19571fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartition\n",
    "df = df.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of partitions\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Repartition according to Column value`\n",
    "\n",
    "---\n",
    "\n",
    "We can also repartition by columns.\n",
    "\n",
    "For example, let’s run the following code to repartition the data by column `Country`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6b5cca0b-9f43-4ee5-8d4e-86f445abf40e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartition\n",
    "df = df.repartition(\"Country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of partitions\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above scripts will create 200 partitions (Spark by default creates 200 partitions). Only two will contain the data:\n",
    "- one partition stores data for CN country\n",
    "- second partition stores data for AU country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Check data per partition`\n",
    "\n",
    "* [spark_partition_id](https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.spark_partition_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition id\n",
    "from pyspark.sql.functions  import spark_partition_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Country</th><th>Date</th><th>Amount</th><th>SPARK_PARTITION_ID()</th></tr>\n",
       "<tr><td>CN</td><td>2019-01-01</td><td>10</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-02</td><td>11</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-03</td><td>12</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-04</td><td>13</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-05</td><td>14</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-06</td><td>15</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-07</td><td>16</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-08</td><td>17</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-09</td><td>18</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-10</td><td>19</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-11</td><td>20</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-12</td><td>21</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-13</td><td>22</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-14</td><td>23</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-15</td><td>24</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-16</td><td>25</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-17</td><td>26</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-18</td><td>27</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-19</td><td>28</td><td>28</td></tr>\n",
       "<tr><td>CN</td><td>2019-01-20</td><td>29</td><td>28</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-------+----------+------+--------------------+\n",
       "|Country|      Date|Amount|SPARK_PARTITION_ID()|\n",
       "+-------+----------+------+--------------------+\n",
       "|     CN|2019-01-01|    10|                  28|\n",
       "|     CN|2019-01-02|    11|                  28|\n",
       "|     CN|2019-01-03|    12|                  28|\n",
       "|     CN|2019-01-04|    13|                  28|\n",
       "|     CN|2019-01-05|    14|                  28|\n",
       "|     CN|2019-01-06|    15|                  28|\n",
       "|     CN|2019-01-07|    16|                  28|\n",
       "|     CN|2019-01-08|    17|                  28|\n",
       "|     CN|2019-01-09|    18|                  28|\n",
       "|     CN|2019-01-10|    19|                  28|\n",
       "|     CN|2019-01-11|    20|                  28|\n",
       "|     CN|2019-01-12|    21|                  28|\n",
       "|     CN|2019-01-13|    22|                  28|\n",
       "|     CN|2019-01-14|    23|                  28|\n",
       "|     CN|2019-01-15|    24|                  28|\n",
       "|     CN|2019-01-16|    25|                  28|\n",
       "|     CN|2019-01-17|    26|                  28|\n",
       "|     CN|2019-01-18|    27|                  28|\n",
       "|     CN|2019-01-19|    28|                  28|\n",
       "|     CN|2019-01-20|    29|                  28|\n",
       "+-------+----------+------+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Partition id for each record\n",
    "df.select(\"*\", spark_partition_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|SPARK_PARTITION_ID()|  count|\n",
      "+--------------------+-------+\n",
      "|                  28|1000000|\n",
      "|                  64|1000000|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count record per partition\n",
    "df.groupBy(spark_partition_id()).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
