{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><h1>  Creating Dataframes  </h1></center>\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will learn how, to create dataframes, we can create a dataframes using the following methods.\n",
    "\n",
    " - Using an RDD\n",
    " - Using Collections\n",
    " - Using a CSV File\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### `Importing the Required Libraies`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://gw02.itversity.com:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.2.6.5.0-292</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x707af0b2fb70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "---\n",
    "\n",
    "#### `Spark DataFrame from an RDD`\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "students_data_rdd = sc.textFile('data/module_8_students_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data type\n",
    "type(students_data_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['101 A Rohit Gurgaon 65 77 43 66 87',\n",
       " '102 B Akansha Delhi 55 46 24 66 77',\n",
       " '103 A Himanshu Faridabad 75 38 84 38 58',\n",
       " '104 A Ekta Delhi 85 84 39 58 85',\n",
       " '105 B Deepanshu Gurgaon 34 55 56 23 66',\n",
       " '106 B Ayush Delhi 66 62 98 74 87',\n",
       " '107 B Aditi Delhi 76 83 75 38 58',\n",
       " '108 A Sahil Faridabad 55 32 43 56 66',\n",
       " '109 A Krati Delhi 34 53 25 67 75']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students_data_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize records\n",
    "students_data_rdd = students_data_rdd.map(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['101', 'A', 'Rohit', 'Gurgaon', '65', '77', '43', '66', '87'],\n",
       " ['102', 'B', 'Akansha', 'Delhi', '55', '46', '24', '66', '77'],\n",
       " ['103', 'A', 'Himanshu', 'Faridabad', '75', '38', '84', '38', '58'],\n",
       " ['104', 'A', 'Ekta', 'Delhi', '85', '84', '39', '58', '85'],\n",
       " ['105', 'B', 'Deepanshu', 'Gurgaon', '34', '55', '56', '23', '66'],\n",
       " ['106', 'B', 'Ayush', 'Delhi', '66', '62', '98', '74', '87'],\n",
       " ['107', 'B', 'Aditi', 'Delhi', '76', '83', '75', '38', '58'],\n",
       " ['108', 'A', 'Sahil', 'Faridabad', '55', '32', '43', '56', '66'],\n",
       " ['109', 'A', 'Krati', 'Delhi', '34', '53', '25', '67', '75']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students_data_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from rdd\n",
    "students_data_dataframe = students_data_rdd.toDF([\"roll_no\",\n",
    "                                                  \"section\",\n",
    "                                                  \"name\",\n",
    "                                                  \"city\",\n",
    "                                                  \"subject1\",\n",
    "                                                  \"subject2\",\n",
    "                                                  \"subject3\",\n",
    "                                                  \"subject4\",\n",
    "                                                  \"subject5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+---------+--------+--------+--------+--------+--------+\n",
      "|roll_no|section|     name|     city|subject1|subject2|subject3|subject4|subject5|\n",
      "+-------+-------+---------+---------+--------+--------+--------+--------+--------+\n",
      "|    101|      A|    Rohit|  Gurgaon|      65|      77|      43|      66|      87|\n",
      "|    102|      B|  Akansha|    Delhi|      55|      46|      24|      66|      77|\n",
      "|    103|      A| Himanshu|Faridabad|      75|      38|      84|      38|      58|\n",
      "|    104|      A|     Ekta|    Delhi|      85|      84|      39|      58|      85|\n",
      "|    105|      B|Deepanshu|  Gurgaon|      34|      55|      56|      23|      66|\n",
      "|    106|      B|    Ayush|    Delhi|      66|      62|      98|      74|      87|\n",
      "|    107|      B|    Aditi|    Delhi|      76|      83|      75|      38|      58|\n",
      "|    108|      A|    Sahil|Faridabad|      55|      32|      43|      56|      66|\n",
      "|    109|      A|    Krati|    Delhi|      34|      53|      25|      67|      75|\n",
      "+-------+-------+---------+---------+--------+--------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display dataframe\n",
    "students_data_dataframe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `Spark DataFrame from Collections`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collection\n",
    "sample_data = [\n",
    "    (101, \"A\", \"Rohit\",    \"Gurugram\"),\n",
    "    (102, \"B\", \"Akansha\",  \"Delhi\"),\n",
    "    (103, \"A\", \"Himanshu\", \"Faridabad\"),\n",
    "    (104, \"A\", \"Ekta\",     \"Delhi\"),\n",
    "    (105, \"B\", \"Ayush\",    \"Delhi\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from collection\n",
    "dataframe_from_collections = spark.createDataFrame(data=sample_data,schema=[\"roll_no\",\n",
    "                                                                            \"section\",\n",
    "                                                                            \"name\",\n",
    "                                                                            \"city\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+---------+\n",
      "|roll_no|section|    name|     city|\n",
      "+-------+-------+--------+---------+\n",
      "|    101|      A|   Rohit| Gurugram|\n",
      "|    102|      B| Akansha|    Delhi|\n",
      "|    103|      A|Himanshu|Faridabad|\n",
      "|    104|      A|    Ekta|    Delhi|\n",
      "|    105|      B|   Ayush|    Delhi|\n",
      "+-------+-------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display dataframe\n",
    "dataframe_from_collections.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `Spark DataFrame from CSV file`\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pyspark.sql.types as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema or dataframe\n",
    "my_schema = tp.StructType([\n",
    "    tp.StructField(name= \"roll_no\", dataType= tp.IntegerType()),\n",
    "    tp.StructField(name= \"section\", dataType= tp.StringType()),\n",
    "    tp.StructField(name= \"name\",    dataType= tp.StringType()),\n",
    "    tp.StructField(name= \"city\",    dataType= tp.StringType()),\n",
    "    tp.StructField(name= \"subject1\",dataType= tp.IntegerType()),\n",
    "    tp.StructField(name= \"subject2\",dataType= tp.IntegerType()),\n",
    "    tp.StructField(name= \"subject3\",dataType= tp.IntegerType()),\n",
    "    tp.StructField(name= \"subject4\",dataType= tp.IntegerType()),\n",
    "    tp.StructField(name= \"subject5\",dataType= tp.IntegerType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "df_csv_schema = spark.read.csv(\"data/module_8_students_data.csv\",\n",
    "                                             header=False, \n",
    "                                             schema=my_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check type\n",
    "type(df_csv_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+---------+--------+--------+--------+--------+--------+\n",
      "|roll_no|section|     name|     city|subject1|subject2|subject3|subject4|subject5|\n",
      "+-------+-------+---------+---------+--------+--------+--------+--------+--------+\n",
      "|    101|      A|    Rohit|  Gurgaon|      65|      77|      43|      66|      87|\n",
      "|    102|      B|  Akansha|    Delhi|      55|      46|      24|      66|      77|\n",
      "|    103|      A| Himanshu|Faridabad|      75|      38|      84|      38|      58|\n",
      "|    104|      A|     Ekta|    Delhi|      85|      84|      39|      58|      85|\n",
      "|    105|      B|Deepanshu|  Gurgaon|      34|      55|      56|      23|      66|\n",
      "|    106|      B|    Ayush|    Delhi|      66|      62|      98|      74|      87|\n",
      "|    107|      B|    Aditi|    Delhi|      76|      83|      75|      38|      58|\n",
      "|    108|      A|    Sahil|Faridabad|      55|      32|      43|      56|      66|\n",
      "|    109|      A|    Krati|    Delhi|      34|      53|      25|      67|      75|\n",
      "+-------+-------+---------+---------+--------+--------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display dataframe\n",
    "df_csv_schema.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `Spark DataFrame from CSV file - with inferSchema`\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with inferSchema\n",
    "df_csv_infer = spark.read.csv(\"data/module_8_students_data.csv\",\n",
    "                                             header=False, \n",
    "                                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check type\n",
    "type(df_csv_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+---------+---+---+---+---+---+\n",
      "|_c0|_c1|      _c2|      _c3|_c4|_c5|_c6|_c7|_c8|\n",
      "+---+---+---------+---------+---+---+---+---+---+\n",
      "|101|  A|    Rohit|  Gurgaon| 65| 77| 43| 66| 87|\n",
      "|102|  B|  Akansha|    Delhi| 55| 46| 24| 66| 77|\n",
      "|103|  A| Himanshu|Faridabad| 75| 38| 84| 38| 58|\n",
      "|104|  A|     Ekta|    Delhi| 85| 84| 39| 58| 85|\n",
      "|105|  B|Deepanshu|  Gurgaon| 34| 55| 56| 23| 66|\n",
      "|106|  B|    Ayush|    Delhi| 66| 62| 98| 74| 87|\n",
      "|107|  B|    Aditi|    Delhi| 76| 83| 75| 38| 58|\n",
      "|108|  A|    Sahil|Faridabad| 55| 32| 43| 56| 66|\n",
      "|109|  A|    Krati|    Delhi| 34| 53| 25| 67| 75|\n",
      "+---+---+---------+---------+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display dataframe\n",
    "df_csv_infer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `Rename DataFrame columns`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns method 1\n",
    "df_csv_infer2 = df_csv_infer.withColumnRenamed(\"_c0\",\"roll_no\")\\\n",
    "                            .withColumnRenamed(\"_c1\",\"section\")\\\n",
    "                            .withColumnRenamed(\"_c2\",\"name\")\\\n",
    "                            .withColumnRenamed(\"_c3\",\"city\")\\\n",
    "                            .withColumnRenamed(\"_c4\",\"section_1\")\\\n",
    "                            .withColumnRenamed(\"_c5\",\"section_2\")\\\n",
    "                            .withColumnRenamed(\"_c6\",\"section_3\")\\\n",
    "                            .withColumnRenamed(\"_c7\",\"section_4\")\\\n",
    "                            .withColumnRenamed(\"_c8\",\"section_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|roll_no|section|     name|     city|section_1|section_2|section_3|section_4|section_5|\n",
      "+-------+-------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|    101|      A|    Rohit|  Gurgaon|       65|       77|       43|       66|       87|\n",
      "|    102|      B|  Akansha|    Delhi|       55|       46|       24|       66|       77|\n",
      "|    103|      A| Himanshu|Faridabad|       75|       38|       84|       38|       58|\n",
      "|    104|      A|     Ekta|    Delhi|       85|       84|       39|       58|       85|\n",
      "|    105|      B|Deepanshu|  Gurgaon|       34|       55|       56|       23|       66|\n",
      "|    106|      B|    Ayush|    Delhi|       66|       62|       98|       74|       87|\n",
      "|    107|      B|    Aditi|    Delhi|       76|       83|       75|       38|       58|\n",
      "|    108|      A|    Sahil|Faridabad|       55|       32|       43|       56|       66|\n",
      "|    109|      A|    Krati|    Delhi|       34|       53|       25|       67|       75|\n",
      "+-------+-------+---------+---------+---------+---------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display datframe\n",
    "df_csv_infer2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns method 2\n",
    "column_names = [\"roll_no\",\"section\",\"name\",\"city\",\"section_1\",\"section_2\",\"section_3\",\"section_4\",\"section_5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df_csv_infer3 = df_csv_infer.toDF(*column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|roll_no|section|     name|     city|section_1|section_2|section_3|section_4|section_5|\n",
      "+-------+-------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|    101|      A|    Rohit|  Gurgaon|       65|       77|       43|       66|       87|\n",
      "|    102|      B|  Akansha|    Delhi|       55|       46|       24|       66|       77|\n",
      "|    103|      A| Himanshu|Faridabad|       75|       38|       84|       38|       58|\n",
      "|    104|      A|     Ekta|    Delhi|       85|       84|       39|       58|       85|\n",
      "|    105|      B|Deepanshu|  Gurgaon|       34|       55|       56|       23|       66|\n",
      "|    106|      B|    Ayush|    Delhi|       66|       62|       98|       74|       87|\n",
      "|    107|      B|    Aditi|    Delhi|       76|       83|       75|       38|       58|\n",
      "|    108|      A|    Sahil|Faridabad|       55|       32|       43|       56|       66|\n",
      "|    109|      A|    Krati|    Delhi|       34|       53|       25|       67|       75|\n",
      "+-------+-------+---------+---------+---------+---------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display dataframe\n",
    "df_csv_infer3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
